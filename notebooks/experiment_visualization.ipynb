{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import orbax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from src.algorithms.value_iteration_and_prediction import general_value_iteration\n",
    "from train_stochastic_bilevel_opt import environment_setup\n",
    "from visualization_functions import plot_UL_rewards, plot_incentive_grid\n",
    "\n",
    "# Parameters for plotting\n",
    "linewidth = 3\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        'font.size': 26,\n",
    "        'text.usetex': True,\n",
    "        'axes.linewidth': linewidth,\n",
    "        'xtick.major.width': linewidth,\n",
    "        'ytick.major.width': linewidth,\n",
    "        'xtick.major.size': 2*linewidth,\n",
    "        'ytick.major.size': 2*linewidth,\n",
    "        'axes.prop_cycle': plt.cycler(color=plt.cm.Dark2.colors),\n",
    "        \"lines.linewidth\": linewidth,\n",
    "    }\n",
    ")\n",
    "colors = plt.cm.Dark2.colors\n",
    "save_figures = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir = \"../data/results/4Rooms_grid_search_lambda_0_001\"\n",
    "rolling_window = 100\n",
    "plot_top = 10\n",
    "\n",
    "grid_search_outputs = {}\n",
    "grid_search_params = {}\n",
    "grid_search_train_states = {}\n",
    "summary_dfs = {}\n",
    "\n",
    "with open(f\"{dir}/config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(config)\n",
    "rng = jax.random.PRNGKey(config[\"random_seed\"])\n",
    "\n",
    "# Environment Setup\n",
    "print(\"Environment Setup\")\n",
    "env, env_params, incentive_train_state, config_incentive = environment_setup(rng, config)\n",
    "\n",
    "algorithms = [\"bilevel\", \"benchmark\", \"zero_order\"]\n",
    "for algo in algorithms:\n",
    "    try:\n",
    "        print(f\"\\n--- {algo} ---\")\n",
    "        with open(f\"{dir}/metrics_{algo}.pkl\" if algo != \"bilevel\" else f\"{dir}/metrics_on_policy.pkl\", \"rb\") as f:\n",
    "            outputs = pickle.load(f)\n",
    "        grid_search_outputs[algo] = outputs\n",
    "        with open(f\"{dir}/grid_search_{algo}.pkl\" if algo != \"bilevel\" else f\"{dir}/grid_search.pkl\", \"rb\") as f:\n",
    "            grid_params = pd.DataFrame(pickle.load(f))\n",
    "        grid_search_params[algo] = grid_params\n",
    "        print(\"Output shape: \", outputs[\"UL_initial_value\"].shape)\n",
    "        print(\"Grid param keys: \", grid_search_params[algo].columns)\n",
    "\n",
    "        # Load orbax checkpoint\n",
    "        ckpt_path = f\"{dir}/checkpoint_incentive_{algo}\" if algo != \"bilevel\" else f\"{dir}/checkpoint_incentive_on_policy\"\n",
    "        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        grid_search_train_states[algo] = orbax_checkpointer.restore(ckpt_path)\n",
    "        # print(incentive_train_state)\n",
    "        print(\"Incentive Train State Loaded\")\n",
    "    except:\n",
    "        print(f\"Failed to load {algo}\")\n",
    "        continue\n",
    "\n",
    "    df = grid_params.copy()\n",
    "    df[\"UL_initial_value\"] = jnp.mean(outputs[\"UL_initial_value\"][:, -1000:], axis=1)\n",
    "    df = df.set_index(list(grid_params.keys()))\n",
    "    df = df.groupby(list(grid_params.keys())).mean()\n",
    "    df = df.sort_values(\"UL_initial_value\", ascending=False)\n",
    "    display(df)\n",
    "    summary_dfs[algo] = df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance Table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the best parameters for each reg_lambda\n",
    "best_parameters_for_reg_lambda = {}\n",
    "for algo in algorithms:\n",
    "    df_grid_params = grid_search_params[algo]\n",
    "    grouped = df_grid_params.groupby(list(df_grid_params.columns))\n",
    "    df = summary_dfs[algo]\n",
    "    best_params = {}\n",
    "    for reg_lambda in jnp.unique(df_grid_params[\"incentive_reg_grid\"].values):\n",
    "        reg_lambda = float(reg_lambda)\n",
    "        df_selected = df.loc[(slice(None), reg_lambda), :]\n",
    "        best_params[reg_lambda] = df_selected.index[0]\n",
    "    best_parameters_for_reg_lambda[algo] = best_params\n",
    "\n",
    "# Create a summary table\n",
    "df_summary_mean = pd.DataFrame()\n",
    "df_summary_std_error = pd.DataFrame()\n",
    "for reg_lambda in jnp.unique(grid_params[\"incentive_reg_grid\"].values):\n",
    "    reg_lambda = float(reg_lambda)\n",
    "    for algo in algorithms:\n",
    "        df_grid_params = grid_search_params[algo]\n",
    "        outputs = grid_search_outputs[algo]\n",
    "        idx = (df_grid_params == best_parameters_for_reg_lambda[algo][reg_lambda]).all(1).values\n",
    "        UL_estimate = jnp.mean(outputs[\"UL_initial_value\"][idx, -1000:], -1)\n",
    "        df_summary_mean.loc[reg_lambda, algo] = jnp.mean(UL_estimate)  # Average across seeds\n",
    "        df_summary_std_error.loc[reg_lambda, algo] = jnp.std(UL_estimate)/jnp.sqrt(UL_estimate.shape[0])  # Standard error across seeds\n",
    "print(\"Average Upper-Level Value (Last 1000 steps)\")\n",
    "display(df_summary_mean)\n",
    "print(\"Std. Error Upper-Level Value (Last 1000 steps)\")\n",
    "display(df_summary_std_error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize the learning curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_lim = {\n",
    "    1: (0.2, 1.1),\n",
    "    2: (0.0, 1.4),\n",
    "    3: (0.0, 0.8),\n",
    "    4: (0.0, 0.8),\n",
    "    5: (0.0, 0.7),\n",
    "}\n",
    "# y_lim = {\n",
    "#     1: (0.1, 1.1),\n",
    "#     2: (0.0, 0.8),\n",
    "#     3: (0.0, 0.8),\n",
    "#     4: (-0.5, 0.6),\n",
    "#     5: (-0.5, 0.6),\n",
    "# }\n",
    "# y_lim = {\n",
    "#     1: (0.2, 1.4),\n",
    "#     2: (0.0, 1.4),\n",
    "#     3: (0.0, 1.4),\n",
    "#     4: (-0.5, 1.2),\n",
    "#     5: (-0.5, 1.2),\n",
    "# }\n",
    "\n",
    "for reg_lambda in jnp.unique(grid_params[\"incentive_reg_grid\"].values):\n",
    "    reg_lambda = float(reg_lambda)\n",
    "    print(f\"\\n--- Reg_lambda: {reg_lambda} ---\")\n",
    "    input_data = {}\n",
    "    for algo in algorithms:\n",
    "        df_grid_params = grid_search_params[algo]\n",
    "        outputs = grid_search_outputs[algo]\n",
    "        idx = (df_grid_params == best_parameters_for_reg_lambda[algo][reg_lambda]).all(1).values  # Shape: (n_grid_params,)\n",
    "        input_data[algo] = outputs[\"UL_initial_value\"][idx]\n",
    "\n",
    "    plot_UL_rewards(\n",
    "        input_data,\n",
    "        figsize=(12, 6),\n",
    "        rolling_window=rolling_window,\n",
    "        savefig_path=f\"{dir}/UL_rewards_grid_search_reg_lambda_{reg_lambda}.pdf\" if save_figures else None,\n",
    "        xlim=(0, 10_000),\n",
    "        ylim=y_lim[reg_lambda] if reg_lambda in y_lim else None,\n",
    "        legend_position={\n",
    "            \"loc\": \"lower center\",\n",
    "            \"bbox_to_anchor\": (0.5, -0.02),\n",
    "            \"ncol\": 3,\n",
    "        },\n",
    "        legend_names = {\n",
    "            \"bilevel\": r\"\\textsc{HPGD}\",\n",
    "            \"benchmark\": r\"\\textsc{AMD}\",\n",
    "            \"zero_order\": r\"\\textsc{Zero-order}\",\n",
    "        },\n",
    "        line_styles = {\n",
    "            \"bilevel\": \"-\",\n",
    "            \"benchmark\": \"--\",\n",
    "            \"zero_order\": (0, (2, 4, 2, 4)),\n",
    "        },\n",
    "        algo_colors={\n",
    "            \"bilevel\": colors[0],\n",
    "            \"benchmark\": colors[1],\n",
    "            \"zero_order\": colors[2],\n",
    "        },\n",
    "        zorder={\n",
    "            \"bilevel\": 3,\n",
    "            \"benchmark\": 2,\n",
    "            \"zero_order\": 1,\n",
    "        }\n",
    "    )\n",
    "    plt.close()\n",
    "plt.clf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize the penalty maps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_idx = 0\n",
    "from src.environments.ConfigurableFourRooms import map_project\n",
    "for reg_lambda in jnp.unique(grid_params[\"incentive_reg_grid\"].values):\n",
    "    reg_lambda = float(reg_lambda)\n",
    "    config_tmp = config.copy()\n",
    "    config_tmp[\"upper_optimisation\"][\"incentive_reg_param\"] = 1.0\n",
    "    print(f\"\\n--- Reg_lambda: {reg_lambda} ---\")\n",
    "    input_data = {}\n",
    "    for algo in algorithms:\n",
    "        df_grid_params = grid_search_params[algo]\n",
    "        train_state = grid_search_train_states[algo]\n",
    "        idx = (df_grid_params == best_parameters_for_reg_lambda[algo][reg_lambda]).all(1).values  # Shape: (n_grid_params,)\n",
    "        input_data[algo] = jax.tree_map(lambda x: jnp.array(x)[idx], train_state[\"params\"])\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(input_data), figsize=(21, 7), constrained_layout=True)\n",
    "    label_names = {\n",
    "        \"bilevel\": r\"\\textsc{HPGD}\",\n",
    "        \"benchmark\": r\"\\textsc{AMD}\",\n",
    "        \"zero_order\": r\"\\textsc{Zero-order}\",\n",
    "    }\n",
    "    for i, (algo, incentive_train_state) in enumerate(input_data.items()):\n",
    "        print(f\"\\n--- {algo} ---\")\n",
    "        axes[i].set_title(label_names[algo])\n",
    "        pcm = plot_incentive_grid(\n",
    "            env,\n",
    "            env_params,\n",
    "            incentive_train_state[\"params\"][\"weights\"][seed_idx],\n",
    "            config_incentive[\"coordinates\"],\n",
    "            config_tmp,\n",
    "            verbose=False,\n",
    "            plot_input=(fig, axes[i]),\n",
    "            cmap=\"PuRd_r\",\n",
    "        )\n",
    "\n",
    "        # Add policy steps to the map\n",
    "        init_probs = env.state_initialization_distribution(env_params.state_initialization_params).probs\n",
    "        init_probs_mask = init_probs > 1e-8\n",
    "        env_params_viz = env_params.replace(\n",
    "            incentive_params=jax.tree_map(lambda x: x[seed_idx], incentive_train_state)\n",
    "        )\n",
    "        config_lower_level = config[\"lower_optimisation\"]\n",
    "        q_final, _ = general_value_iteration(\n",
    "            env,\n",
    "            env_params_viz,\n",
    "            config_lower_level[\"discount_factor\"],\n",
    "            n_policy_iter=config_lower_level[\"n_policy_iter\"],\n",
    "            n_value_iter=config_lower_level[\"n_value_iter\"],\n",
    "            regularization=config_lower_level[\"regularization\"],\n",
    "            reg_lambda=config_lower_level[\"reg_lambda\"],\n",
    "            return_q_value=True,\n",
    "        )\n",
    "        br_policy = jax.nn.softmax(q_final/config_lower_level[\"reg_lambda\"], axis=-1)  # Shape: (n_goals, n_states, n_actions)\n",
    "        for j in range(len(env.available_goals)):\n",
    "            goal_pos = jnp.array(config[\"environment\"][\"available_goals\"][j])\n",
    "            pos = env.available_init_pos[init_probs_mask][0]\n",
    "            # Add the policy trajectories to the map\n",
    "            for _ in range(30):\n",
    "                try:\n",
    "                    pos_idx = jnp.where(jnp.all(env.coords == pos[None, :], 1))[0][0]\n",
    "                except:\n",
    "                    print(pos)\n",
    "                    break\n",
    "                action_sort = jnp.argsort(br_policy[j, pos_idx])[::-1]\n",
    "                for action in action_sort:\n",
    "                    action_direction = env.directions[action]\n",
    "                    new_pos = map_project(env.env_map, pos, pos + action_direction)\n",
    "                    if not jnp.all(new_pos == pos):\n",
    "                        break\n",
    "                color = \"gray\"\n",
    "                axes[i].arrow(\n",
    "                    pos[1],\n",
    "                    pos[0],\n",
    "                    action_direction[1]/2.0,\n",
    "                    action_direction[0]/2.0,\n",
    "                    head_width=0.1,\n",
    "                    head_length=0.1,\n",
    "                    linewidth=linewidth,\n",
    "                    fc=color,\n",
    "                    ec=color,\n",
    "                    alpha=0.9,\n",
    "                )\n",
    "                pos = new_pos\n",
    "                if jnp.all(pos == goal_pos):\n",
    "                    break\n",
    "\n",
    "\n",
    "        # Add goal and initial position annotations\n",
    "        for j in range(len(env.available_goals)):\n",
    "            goal_pos = config[\"environment\"][\"available_goals\"][j]\n",
    "            axes[i].annotate(\n",
    "                rf\"$\\textbf{{G}}^{j+1}$\",\n",
    "                xy=(goal_pos[1], goal_pos[0]),\n",
    "                xycoords=\"data\",\n",
    "                xytext=(goal_pos[1] - 0.3, goal_pos[0] + 0.25),\n",
    "            )\n",
    "        init_states_counter = 1\n",
    "        for pos in env.available_init_pos[init_probs_mask]:\n",
    "            axes[i].annotate(\n",
    "                rf\"$\\textbf{{S^{init_states_counter}}}$\" if sum(init_probs_mask) > 1 else rf\"$\\textbf{{S}}$\",\n",
    "                # fontsize=20,\n",
    "                weight=\"bold\",\n",
    "                xy=(pos[1], pos[0]),\n",
    "                xycoords=\"data\",\n",
    "                xytext=(pos[1]-0.3, pos[0]+0.25),\n",
    "\n",
    "            )\n",
    "            init_states_counter += 1\n",
    "        pos = config[\"upper_optimisation\"][\"reward_function\"][\"target_state\"]\n",
    "        axes[i].annotate(\n",
    "            rf\"$\\textbf{{+1}}$\" if config[\"upper_optimisation\"][\"reward_function\"][\"type\"] == \"positive\" else rf\"$\\textbf{{-1}}$\",\n",
    "            # fontsize=20,\n",
    "            weight=\"bold\",\n",
    "            xy=(pos[1], pos[0]),\n",
    "            xycoords=\"data\",\n",
    "            xytext=(pos[1]-0.3, pos[0]+0.25),\n",
    "        )\n",
    "        # Calculate the upper_level_value\n",
    "        unused_pct = 100*jax.nn.softmax(incentive_train_state['params']['weights'][seed_idx])[-1]\n",
    "        print(f\"Unused percentage: {unused_pct:.4f}%\")\n",
    "    cbar = fig.colorbar(pcm, ax=axes.ravel().tolist(), shrink=0.8)\n",
    "    cbar.outline.set_visible(False)\n",
    "    plt.subplots_adjust(right=0.98)\n",
    "    if save_figures:\n",
    "        fig.savefig(f\"{dir}/incentive_grid_grid_search_seed_{seed_idx}_reg_lambda_{reg_lambda}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
